{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Dowload ShapeNetPart dataset from https://shapenet.cs.stanford.edu/media/shapenetcore_partanno_segmentation_benchmark_v0_normal.zip\n",
    "\n",
    "In the Data folder, create a folder called ShapeNet and unzip the downloaded file in it.\n",
    "\n",
    "Rename the folder in ShapeNet to shape_data (this is because the train test split json files use \"shape_data\" as the directory name).\n",
    "\n",
    "Note: We are no longer using the train test split json files. We just did an 80/20 split on the data ourselves.\n",
    "\n",
    "The folder structure should be as follows:\n",
    "\n",
    "Data\n",
    "    ShapeNet\n",
    "        shape_data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Folders For ShapeNetPart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_names():\n",
    "    \"\"\"\n",
    "    Get the category names from the file synsetoffset2category.txt\n",
    "    :return: a list of category names\n",
    "    \"\"\"\n",
    "    category_names = []\n",
    "    category_synset_ids = []\n",
    "    with open('../Data/ShapeNet/shape_data/synsetoffset2category.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            category_names.append(line.split()[0])\n",
    "            category_synset_ids.append(line.split()[1])\n",
    "\n",
    "    return category_names, category_synset_ids\n",
    "\n",
    "def get_category_synset_id(category_name):\n",
    "    \"\"\"\n",
    "    Given a category name, return the corresponding synset id\n",
    "    \"\"\"\n",
    "    category_names, category_synset_ids = get_category_names()\n",
    "\n",
    "    # Check if the category name is in the list of category names\n",
    "    if category_name not in category_names:\n",
    "        raise ValueError('Category name not in list of category names')\n",
    "\n",
    "    return category_synset_ids[category_names.index(category_name)]\n",
    "\n",
    "# Rename folders to category names\n",
    "def rename_folders_to_category_names():\n",
    "    \"\"\"\n",
    "    Rename folders to category names\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    category_names, category_synset_ids = get_category_names()\n",
    "    for category_name, category_synset_id in zip(category_names, category_synset_ids):\n",
    "        try:\n",
    "            os.rename('../Data/ShapeNet/shape_data/' + category_synset_id + '/', '../Data/ShapeNet/shape_data/' + category_name + '/')\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_folders_to_category_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ShapeNetV2 binvox files for ShapeNetPart dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a directory called ShapeNetV2 in the Data folder. In this directory store each zip file from the huggingface dataset that matches the 16 category names from ShapeNetPart. (see ShapeNet/shape_data/synsetoffset2category.txt for the category names and synset ids)\n",
    "Within the directory create an excludefiles.txt file and add the following lines to it:\n",
    "\n",
    "*.binvox\n",
    "*.mtl\n",
    "*.png\n",
    "*.jpg\n",
    "images\n",
    "screenshots\n",
    "untitled\n",
    "\n",
    "This will save considerable space as the zip files contain a lot of unnecessary files for our purposes.\n",
    "\n",
    "Make sure that 7zip is installed on your system and that the 7z executable is in your PATH variable (see https://www.7-zip.org/download.html).\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "\n",
    "zips = os.listdir('../Data/ShapeNetV2/')\n",
    "zips = [zip for zip in zips if zip.endswith('.zip')]\n",
    "os.chdir('../Data/ShapeNetV2/')\n",
    "\n",
    "# Unzip the zip files\n",
    "for zip in zips:\n",
    "    subprocess.run(['7z', 'x', zip, '-xr@excludefiles.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The directory structure should be as follows:\n",
    "\n",
    "ShapeNetV2\n",
    "    {category synset id}\n",
    "        {model hash}\n",
    "            models\n",
    "                model_normalized.solid.binvox\n",
    "\n",
    "The binvox files are named model_normalized.solid.binvox. We want to rename them to {model hash}.binvox and move them to the ShapeNet folder under the corresponding category name.\n",
    "\"\"\"\n",
    "\n",
    "def move_binvox_files():\n",
    "    \"\"\"\n",
    "    Rename binvox files and move them to the ShapeNet folder under the corresponding category name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    category_names, category_synset_ids = get_category_names()\n",
    "    for category_name, category_synset_id in zip(category_names, category_synset_ids):\n",
    "        \n",
    "        # Get the list of model hashes for the category\n",
    "        model_hashes = os.listdir('../Data/ShapeNetV2/' + category_synset_id + '/')\n",
    "        part_hashes = os.listdir('../Data/ShapeNet/shape_data/' + category_name + '/')\n",
    "\n",
    "        # Get all .txt files in the category folder\n",
    "        part_hashes = [part_hash for part_hash in part_hashes if part_hash.endswith('.txt')]\n",
    "\n",
    "        # Remove .txt from the model hashes\n",
    "        part_hashes = [part_hash.replace('.txt', '') for part_hash in part_hashes]\n",
    "\n",
    "        # Get the intersection of the two lists\n",
    "        model_hashes = list(set(model_hashes).intersection(set(part_hashes)))\n",
    "\n",
    "        for model_hash in model_hashes:\n",
    "            # Rename the obj file and move it to the ShapeNet folder under the corresponding category name\n",
    "            try:\n",
    "                os.rename('../Data/ShapeNetV2/' + category_synset_id + '/' + model_hash + '/models/model_normalized.obj', '../Data/ShapeNet/shape_data/' + category_name + '/' + model_hash + '.obj')\n",
    "                os.rename('../Data/ShapeNetV2/' + category_synset_id + '/' + model_hash + '/models/model_normalized.json', '../Data/ShapeNet/shape_data/' + category_name + '/' + model_hash + '.json')\n",
    "            except FileNotFoundError:\n",
    "                print('File not found: ' + model_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_binvox_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test Splits for ShapeNetPart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 80-20 train-test split\n",
    "def create_test_train():\n",
    "    # Get the category names\n",
    "    category_names, category_synset_ids = get_category_names()\n",
    "\n",
    "    train = {}\n",
    "    test = {}\n",
    "\n",
    "    # Iterate through each folder and create a train split 80%\n",
    "    for category_name, synset_id in zip(category_names, category_synset_ids):\n",
    "\n",
    "        train[category_name] = []\n",
    "        test[category_name] = []\n",
    "        \n",
    "        # Get the list of folders in the category\n",
    "        files = os.listdir('../Data/ShapeNet/shape_data/' + category_name)\n",
    "\n",
    "        # Omit files that contain .obj\n",
    "        files = [file for file in files if '.obj' not in file]\n",
    "\n",
    "        split_range = int(len(files) * 0.8)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        # Iterate through files and create a train test splits\n",
    "        for file in files:\n",
    "            if i < split_range:\n",
    "                train[category_name].append(file[:-4])\n",
    "            else:\n",
    "                test[category_name].append(file[:-4])\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        obj_files = os.listdir('../Data/ShapeNet/shape_data/' + category_name)\n",
    "        obj_files = [file for file in obj_files if '.obj' in file]\n",
    "\n",
    "        # Iterate through dictionaries and find any files that do not have a corresponding .obj file\n",
    "        for file in train[category_name]:\n",
    "            if file.split(\".\")[0] + '.obj' not in obj_files:\n",
    "                train[category_name].remove(file)\n",
    "        for file in test[category_name]:\n",
    "            if file.split(\".\")[0] + '.obj' not in obj_files:\n",
    "                test[category_name].remove(file)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_test_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Metadata File For ShapeNetPart Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the file paths and the corresponding category names\n",
    "def create_metadata_file(train, test):\n",
    "    \"\"\"\n",
    "    Create a metadata file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    category_names, category_synset_ids = get_category_names()\n",
    "    metadata = []\n",
    "\n",
    "    for category_name, synset_id in zip(category_names, category_synset_ids):\n",
    "        i = 0\n",
    "        for file in train[category_name]:\n",
    "            metadata.append([i, category_name, 'train', category_name + '/' + file + '.obj'])\n",
    "            i += 1\n",
    "        for file in test[category_name]:\n",
    "            metadata.append([i, category_name, 'test', category_name + '/' + file + '.obj'])\n",
    "            i += 1\n",
    "\n",
    "    df = pd.DataFrame(metadata, columns=['object_id', 'class', 'split', 'object_path'])\n",
    "    df.to_csv('../Data/ShapeNet/metadata_shapenet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_file(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
