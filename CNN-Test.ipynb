{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "from Scripts import CNN\n",
    "from Scripts import VoxelizeData as vd\n",
    "\n",
    "#%set_env CUDA_LAUNCH_BLOCKING=1\n",
    "#%set_env TORCH_USE_CUDA_DSA=1\n",
    "\n",
    "# Use GPU if available, has issues with these small batch sizes\n",
    "if torch.cuda.is_available(): \n",
    "    dev = torch.device(\"cuda\")\n",
    "else: \n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "#dev = torch.device(\"cpu\")\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 1 of the following 3 cells depending on which dataset you want to use\n",
    "#### Load ModelNet40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = np.load('Data/ModelNet40/ModelNet40Train.npz', allow_pickle=True)\n",
    "\n",
    "train_data = data['data']\n",
    "train_labels = data['labels']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# Load test data\n",
    "data = np.load('Data/ModelNet40/ModelNet40Test.npz', allow_pickle=True)\n",
    "\n",
    "test_data = data['data']\n",
    "test_labels = data['labels']\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "output_file = 'Data/ModelNet40/cnn_results.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ShapeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = np.load('Data/ShapeNet/ShapeNetTrain.npz', allow_pickle=True)\n",
    "\n",
    "train_data = data['data']\n",
    "train_labels = data['labels']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# Load test data\n",
    "data = np.load('Data/ShapeNet/ShapeNetTest.npz', allow_pickle=True)\n",
    "\n",
    "test_data = data['data']\n",
    "test_labels = data['labels']\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "output_file = 'Data/ShapeNet/cnn_results.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Toys4K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = np.load('Data/toys4k/toys4kTrain.npz', allow_pickle=True)\n",
    "\n",
    "train_data = data['data']\n",
    "train_labels = data['labels']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "# Load test data\n",
    "data = np.load('Data/toys4k/toys4kTest.npz', allow_pickle=True)\n",
    "\n",
    "test_data = data['data']\n",
    "test_labels = data['labels']\n",
    "\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "output_file = 'Data/toys4k/cnn_results.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "classes = np.unique(train_labels)\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "\n",
    "# Make a parameter grid for grid search\n",
    "param_grid = {'epochs': [10, 20, 30, 40, 50],\n",
    "        'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n",
    "        'momentum': [0.9, 0.95, 0.99, 0.999],\n",
    "        'weight_decay': [0.01, 0.001, 0.0001, 0.00001],\n",
    "        'decay_iter': [10000, 20000, 40000, 80000]\n",
    "        }\n",
    "\n",
    "# Store index of the best iteration for each metric\n",
    "best_results = {'f1': 0, 'acc': 0, 'mse': 0}\n",
    "\n",
    "# Store the f1 score, accuracy, MSEs, and confusion matrices for each parameter combination, as well as the parameters\n",
    "metrics = {'f1_scores': [], 'accuracies': [], 'mses': [], 'conf_matrices': []}\n",
    "results = {'train': [], 'test': [], 'params': [], 'best': best_results}\n",
    "\n",
    "# Load data in batch sizes\n",
    "train_loader = vd.build_dataloader(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "test_loader = vd.build_dataloader(test_data, test_labels, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Apply grid search using the sklearn ParameterGrid\n",
    "for i, params in enumerate(ParameterGrid(param_grid)):\n",
    "    print(f'iteration {i}: {params}')\n",
    "\n",
    "    # Get the parameters\n",
    "    learning_rate = params['learning_rate']\n",
    "    momentum = params['momentum']\n",
    "    weight_decay = params['weight_decay']\n",
    "    decay_iter = params['decay_iter']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    # Store the metrics for this iteration so we can plot them later\n",
    "    results['test'].append(metrics.copy())\n",
    "    results['train'].append(metrics.copy())\n",
    "\n",
    "    # Store the parameters for this iteration\n",
    "    results['params'].append(params)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CNN.CNN(num_classes)\n",
    "    model = model.float()\n",
    "    model.to(dev)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    batch_number = 0\n",
    "\n",
    "    # Train and test the model for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train CNN\n",
    "        for (vox_grids, vox_labels) in train_loader:\n",
    "            vox_grids = vox_grids.to(dev)\n",
    "            vox_labels = vox_labels.to(dev)\n",
    "\n",
    "            batch_number += 1\n",
    "\n",
    "            if batch_number % decay_iter == 0:\n",
    "                print(\"Decreasing learning rate\")\n",
    "                learning_rate *= 0.1\n",
    "\n",
    "            # print(len(vox_grids))\n",
    "            if (len(vox_grids) < batch_size):\n",
    "                continue\n",
    "\n",
    "            output = model(vox_grids)\n",
    "            loss = criterion(output, vox_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test CNN after training\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total = 0\n",
    "            predictions_train = []\n",
    "            labels_train = []\n",
    "\n",
    "            # Test the model on the training data\n",
    "            for (grids, labels) in train_loader:\n",
    "                if (len(grids) < batch_size):\n",
    "                    continue\n",
    "\n",
    "                grids = grids.to(dev)\n",
    "                labels = labels.to(dev)\n",
    "\n",
    "                output = model(grids)\n",
    "\n",
    "                _, predictions = torch.max(output, 1)\n",
    "\n",
    "                # Determine the number of correct predictions\n",
    "                num_correct += (predictions.cpu() == labels.cpu()).sum().item()\n",
    "                \n",
    "                # Determine the number of total predictions made\n",
    "                total += labels.shape[0]\n",
    "                \n",
    "                # Store the predictions and labels for calculating the metrics\n",
    "                predictions_train += predictions.cpu().tolist()\n",
    "                labels_train += labels.cpu().tolist()\n",
    "            \n",
    "            # Calculate the metrics\n",
    "            accuracy = (num_correct / total) * 100.0\n",
    "            f1 = f1_score(labels_train, predictions_train, average='weighted')\n",
    "            mse = mean_squared_error(labels_train, predictions_train)\n",
    "\n",
    "            # Store the training results\n",
    "            results['train'][i]['accuracies'].append(accuracy)\n",
    "            results['train'][i]['f1_scores'].append(f1)\n",
    "            results['train'][i]['mses'].append(mse)\n",
    "            results['train'][i]['conf_matrices'].append(confusion_matrix(labels_train, predictions_train, labels=[i for i in range(num_classes)]))\n",
    "\n",
    "            # Print final results of the model before moving on to the next parameter combination\n",
    "            if epoch == epochs-1:\n",
    "                print(\"Final Training Accuracy:\", accuracy)\n",
    "                print(\"Final Training F1 Score:\", f1)\n",
    "                print(\"Final Training MSE:\", mse)\n",
    "\n",
    "            num_correct = 0\n",
    "            total = 0\n",
    "\n",
    "            predictions_test = []\n",
    "            labels_test = []\n",
    "\n",
    "            # Test the model on the test data\n",
    "            for (grids, labels) in test_loader:\n",
    "                if (len(grids) < batch_size):\n",
    "                    continue\n",
    "\n",
    "                grids = grids.to(dev)\n",
    "                labels = labels.to(dev)\n",
    "\n",
    "                output = model(grids)\n",
    "\n",
    "                _, predictions = torch.max(output, 1)\n",
    "\n",
    "                # Determine the number of correct predictions\n",
    "                num_correct += (predictions.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "                # Determine the number of total predictions made\n",
    "                total += labels.shape[0]\n",
    "\n",
    "                # Store the predictions and labels for calculating the metrics\n",
    "                predictions_test += predictions.cpu().tolist()\n",
    "                labels_test += labels.cpu().tolist()\n",
    "            \n",
    "            # Calculate the metrics\n",
    "            accuracy = (num_correct / total) * 100.0\n",
    "            f1 = f1_score(labels_test, predictions_test, average='weighted')\n",
    "            mse = mean_squared_error(labels_test, predictions_test)\n",
    "\n",
    "            # Store the results\n",
    "            results['test'][i]['accuracies'].append(accuracy)\n",
    "            results['test'][i]['f1_scores'].append(f1)\n",
    "            results['test'][i]['mses'].append(mse)\n",
    "            results['test'][i]['conf_matrices'].append(confusion_matrix(labels_test, predictions_test, labels=[i for i in range(num_classes)]))\n",
    "\n",
    "            # Print final results of the model before moving on to the next parameter combination\n",
    "            if epoch == epochs-1:\n",
    "                print(\"Final Testing Accuracy:\", accuracy)\n",
    "                print(\"Final Testing F1 Score:\", f1)\n",
    "                print(\"Final Testing MSE:\", mse)\n",
    "\n",
    "            # Take note of the model with the best accuracy on the test set once all epochs are complete\n",
    "            if accuracy > best_acc and epoch == epochs-1:\n",
    "                results['best']['acc'] = i\n",
    "            \n",
    "            # Take note of the model with the best f1 score on the test set once all epochs are complete\n",
    "            if f1_score(labels_test, predictions_test, average='weighted') > best_f1 and epoch == epochs-1:\n",
    "                results['best']['f1'] = i\n",
    "\n",
    "            # Take note of the model with the best MSE on the test set once all epochs are complete\n",
    "            if mean_squared_error(labels_test, predictions_test) < best_mse and epoch == epochs-1:\n",
    "                results['best']['mse'] = i\n",
    "\n",
    "# Convert the results to a JSON string\n",
    "json_string = json.dumps(results, indent=4)\n",
    "\n",
    "# Write the results to a file\n",
    "with open(output_file, 'w') as outfile:\n",
    "    outfile.write(json_string)\n",
    "\n",
    "# line_one = plt.plot(range(len(training_accuracy)), training_accuracy, label='Training Accuracy')\n",
    "# line_two = plt.plot(range(len(testing_accuracy)), testing_accuracy, label='Testing Accuracy')\n",
    "# plt.legend()\n",
    "# plt.ylim(0,100.0)\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epochs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
