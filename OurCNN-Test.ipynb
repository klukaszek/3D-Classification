{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import f1_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "from Scripts import CNN\n",
    "from Scripts import VoxelizeData as vd\n",
    "\n",
    "#%set_env CUDA_LAUNCH_BLOCKING=1\n",
    "#%set_env TORCH_USE_CUDA_DSA=1\n",
    "\n",
    "# Use GPU if available, has issues with these small batch sizes\n",
    "if torch.cuda.is_available(): \n",
    "    dev = torch.device(\"cuda\")\n",
    "else: \n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "#dev = torch.device(\"cpu\")\n",
    "\n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset To Train Model\n",
    "\n",
    "Change **training_set** to any of the following:\n",
    "\n",
    "- ModelNet40\n",
    "- ShapeNet\n",
    "- Toys\n",
    "\n",
    "If you have 16GB of RAM or less, I recommend using the Toys dataset, and then avoiding the cross-dataset evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'chair': 889, 'sofa': 680, 'airplane': 626, 'bookshelf': 572, 'bed': 515, 'vase': 475, 'monitor': 465, 'table': 392, 'toilet': 344, 'bottle': 335, 'mantel': 284, 'tv': 267, 'plant': 240, 'piano': 231, 'desk': 200, 'dresser': 200, 'night': 200, 'car': 197, 'bench': 173, 'glass': 171, 'cone': 167, 'tent': 163, 'guitar': 155, 'flower': 149, 'laptop': 149, 'keyboard': 145, 'curtain': 138, 'sink': 128, 'lamp': 124, 'stairs': 124, 'range': 115, 'door': 109, 'bathtub': 106, 'radio': 104, 'xbox': 103, 'stool': 90, 'person': 88, 'wardrobe': 87, 'cup': 79, 'bowl': 64})\n"
     ]
    }
   ],
   "source": [
    "training_set = \"ModelNet40\"\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = vd.load_dataset(training_set)\n",
    "\n",
    "import collections\n",
    "counter = collections.Counter(train_labels)\n",
    "print(counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Best Parameters Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "iteration 0: {'decay_iter': 10000, 'epochs': 10, 'learning_rate': 0.0001, 'momentum': 0.9, 'weight_decay': 0.01}\n",
      "Final Training Accuracy: 9.028908794788274\n",
      "Final Training F1 Score: 0.01495405116418112\n",
      "Final Training MSE: 254.69645765472313\n",
      "Final Testing Accuracy: 4.058441558441558\n",
      "Final Testing F1 Score: 0.0031657110440261767\n",
      "Final Testing MSE: 259.41112012987014\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type ndarray is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 193\u001b[0m\n\u001b[0;32m    190\u001b[0m                 results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Convert the results to a JSON string\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m json_string \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type ndarray is not JSON serializable"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "classes = np.unique(train_labels)\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "\n",
    "# Make a parameter grid for grid search\n",
    "param_grid = {'epochs': [5, 10],\n",
    "        'learning_rate': [0.01, 0.001, 0.0001],\n",
    "        'momentum': [0.9, 0.95, 0.99, 0.999],\n",
    "        'weight_decay': [0.01, 0.001],\n",
    "        'decay_iter': [10000, 20000, 40000, 80000]\n",
    "        }\n",
    "\n",
    "# Store index of the best iteration for each metric\n",
    "best_results = {'f1': 0, 'acc': 0, 'mse': 0}\n",
    "\n",
    "# Store the f1 score, accuracy, MSEs, and confusion matrices for each parameter combination, as well as the parameters\n",
    "metrics = {'f1_scores': [], 'accuracies': [], 'mses': [], 'conf_matrices': []}\n",
    "results = {'train': [], 'test': [], 'params': [], 'best': best_results}\n",
    "\n",
    "# Load data in batch sizes\n",
    "train_loader = vd.build_dataloader(train_data, train_labels, batch_size=batch_size, shuffle=True)\n",
    "test_loader = vd.build_dataloader(test_data, test_labels, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "best_acc = 0\n",
    "best_f1 = 0\n",
    "best_mse = float('inf')\n",
    "\n",
    "# Apply grid search using the sklearn ParameterGrid\n",
    "for i, params in enumerate(ParameterGrid(param_grid)):\n",
    "    print(f'iteration {i}: {params}')\n",
    "\n",
    "    # Get the parameters\n",
    "    learning_rate = params['learning_rate']\n",
    "    momentum = params['momentum']\n",
    "    weight_decay = params['weight_decay']\n",
    "    decay_iter = params['decay_iter']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    # Store the metrics for this iteration so we can plot them later\n",
    "    results['test'].append(metrics.copy())\n",
    "    results['train'].append(metrics.copy())\n",
    "\n",
    "    # Store the parameters for this iteration\n",
    "    results['params'].append(params)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = CNN.CNN(num_classes)\n",
    "    model = model.float()\n",
    "    model.to(dev)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    batch_number = 0\n",
    "\n",
    "    # Train and test the model for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Train CNN\n",
    "        for (vox_grids, vox_labels) in train_loader:\n",
    "            vox_grids = vox_grids.to(dev)\n",
    "            vox_labels = vox_labels.to(dev)\n",
    "\n",
    "            batch_number += 1\n",
    "\n",
    "            if batch_number % decay_iter == 0:\n",
    "                print(\"Decreasing learning rate\")\n",
    "                learning_rate *= 0.1\n",
    "\n",
    "            # print(len(vox_grids))\n",
    "            if (len(vox_grids) < batch_size):\n",
    "                continue\n",
    "\n",
    "            output = model(vox_grids)\n",
    "            loss = criterion(output, vox_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Test CNN after training\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total = 0\n",
    "            predictions_train = []\n",
    "            labels_train = []\n",
    "\n",
    "            # Test the model on the training data\n",
    "            for (grids, labels) in train_loader:\n",
    "                if (len(grids) < batch_size):\n",
    "                    continue\n",
    "\n",
    "                grids = grids.to(dev)\n",
    "                labels = labels.to(dev)\n",
    "\n",
    "                output = model(grids)\n",
    "\n",
    "                _, predictions = torch.max(output, 1)\n",
    "\n",
    "                # Determine the number of correct predictions\n",
    "                num_correct += (predictions.cpu() == labels.cpu()).sum().item()\n",
    "                \n",
    "                # Determine the number of total predictions made\n",
    "                total += labels.shape[0]\n",
    "                \n",
    "                # Store the predictions and labels for calculating the metrics\n",
    "                predictions_train += predictions.cpu().tolist()\n",
    "                labels_train += labels.cpu().tolist()\n",
    "            \n",
    "            # Calculate the metrics\n",
    "            accuracy = (num_correct / total) * 100.0\n",
    "            f1 = f1_score(labels_train, predictions_train, average='weighted')\n",
    "            mse = mean_squared_error(labels_train, predictions_train)\n",
    "\n",
    "            # Store the training results\n",
    "            results['train'][i]['accuracies'].append(accuracy)\n",
    "            results['train'][i]['f1_scores'].append(f1)\n",
    "            results['train'][i]['mses'].append(mse)\n",
    "            results['train'][i]['conf_matrices'].append(confusion_matrix(labels_train, predictions_train, labels=[i for i in range(num_classes)]))\n",
    "\n",
    "            # Print final results of the model before moving on to the next parameter combination\n",
    "            if epoch == epochs-1:\n",
    "                print(\"Final Training Accuracy:\", accuracy)\n",
    "                print(\"Final Training F1 Score:\", f1)\n",
    "                print(\"Final Training MSE:\", mse)\n",
    "\n",
    "            num_correct = 0\n",
    "            total = 0\n",
    "\n",
    "            predictions_test = []\n",
    "            labels_test = []\n",
    "\n",
    "            # Test the model on the test data\n",
    "            for (grids, labels) in test_loader:\n",
    "                if (len(grids) < batch_size):\n",
    "                    continue\n",
    "\n",
    "                grids = grids.to(dev)\n",
    "                labels = labels.to(dev)\n",
    "\n",
    "                output = model(grids)\n",
    "\n",
    "                _, predictions = torch.max(output, 1)\n",
    "\n",
    "                # Determine the number of correct predictions\n",
    "                num_correct += (predictions.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "                # Determine the number of total predictions made\n",
    "                total += labels.shape[0]\n",
    "\n",
    "                # Store the predictions and labels for calculating the metrics\n",
    "                predictions_test += predictions.cpu().tolist()\n",
    "                labels_test += labels.cpu().tolist()\n",
    "            \n",
    "            # Calculate the metrics\n",
    "            accuracy = (num_correct / total) * 100.0\n",
    "            f1 = f1_score(labels_test, predictions_test, average='weighted')\n",
    "            mse = mean_squared_error(labels_test, predictions_test)\n",
    "\n",
    "            # Store the results\n",
    "            results['test'][i]['accuracies'].append(accuracy)\n",
    "            results['test'][i]['f1_scores'].append(f1)\n",
    "            results['test'][i]['mses'].append(mse)\n",
    "            results['test'][i]['conf_matrices'].append(confusion_matrix(labels_test, predictions_test, labels=[i for i in range(num_classes)]))\n",
    "\n",
    "            # Print final results of the model before moving on to the next parameter combination\n",
    "            if epoch == epochs-1:\n",
    "                print(\"Final Testing Accuracy:\", accuracy)\n",
    "                print(\"Final Testing F1 Score:\", f1)\n",
    "                print(\"Final Testing MSE:\", mse)\n",
    "\n",
    "            # Take note of the model with the best accuracy on the test set once all epochs are complete\n",
    "            if accuracy > best_acc and epoch == epochs-1:\n",
    "                results['best']['acc'] = i\n",
    "            \n",
    "            # Take note of the model with the best f1 score on the test set once all epochs are complete\n",
    "            if f1_score(labels_test, predictions_test, average='weighted') > best_f1 and epoch == epochs-1:\n",
    "                results['best']['f1'] = i\n",
    "\n",
    "            # Take note of the model with the best MSE on the test set once all epochs are complete\n",
    "            if mean_squared_error(labels_test, predictions_test) < best_mse and epoch == epochs-1:\n",
    "                results['best']['mse'] = i\n",
    "\n",
    "# Convert the results to a JSON string\n",
    "json_string = json.dumps(results, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
